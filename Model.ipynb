{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21041, 32])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "# 引入attribute向量\n",
    "attribute_embedding = np.load(\"generateData/Attribute_High_Embedding.npy\")\n",
    "\n",
    "# 引入category向量\n",
    "category_embedding = np.load(\"generateData/Category_Embedding.npy\")\n",
    "\n",
    "# shop特征矩阵\n",
    "shop_matrix = np.concatenate((attribute_embedding, category_embedding), axis=1)\n",
    "padLine = np.zeros([1, attribute_embedding.shape[1]+category_embedding.shape[1]])\n",
    "shop_matrix = np.insert(shop_matrix, 0, values=padLine, axis=0)    # pad 0\n",
    "shop_matrix = torch.from_numpy(shop_matrix)\n",
    "\n",
    "# user历史访问数据\n",
    "user_history_dict = np.load('newData/UserHistoryData.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, degree\n",
    "\n",
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNConv, self).__init__(aggr='mean')  # \"Add\" aggregation.\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        # Step 1: Add self-loops to the adjacency matrix.\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Step 2: Linearly transform node feature matrix.\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Step 3-5: Start propagating messages.\n",
    "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\n",
    "    \n",
    "    def message(self, x_j, edge_index, size):\n",
    "        # x_j has shape [E, out_channels]\n",
    "\n",
    "        # Step 3: Normalize node features.\n",
    "        row, col = edge_index\n",
    "        deg = degree(row, size[0], dtype=x_j.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "\n",
    "        # Step 5: Return new node embeddings.\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shop_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3d37af845a25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_packed_sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0membed_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshop_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mshop_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshop_embed_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshop_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshop_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'shop_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "embed_dim = shop_matrix.shape[1]\n",
    "shop_size, shop_embed_size = shop_matrix.shape[0], shop_matrix.shape[1]\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.shop_size = shop_size\n",
    "        self.shop_embed_size = shop_embed_size\n",
    "        self.shop_embedding = nn.Embedding(self.shop_size, self.shop_embed_size, padding_idx=0)\n",
    "        self.shop_embedding.weight.data.copy_(shop_matrix)\n",
    "        self.shop_embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.conv1 = GCNConv(embed_dim, embed_dim)\n",
    "        self.bn = nn.BatchNorm1d(embed_dim)\n",
    "        self.active = nn.ReLU()\n",
    "        self.conv2 = GCNConv(embed_dim, embed_dim)\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size=embed_dim, hidden_size=embed_dim, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=embed_dim, hidden_size=embed_dim, batch_first=True)\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(2*embed_dim),\n",
    "            nn.Linear(2*embed_dim, embed_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.BatchNorm1d(2*embed_dim),\n",
    "            nn.Linear(2*embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(embed_dim),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(embed_dim, 5)\n",
    "        )\n",
    "        \n",
    "        # attention1参数\n",
    "        self.w_omega = nn.Parameter(torch.Tensor(embed_dim, embed_dim))\n",
    "        self.u_omega = nn.Parameter(torch.Tensor(embed_dim, 1))\n",
    "        nn.init.uniform_(self.w_omega, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.u_omega, -0.1, 0.1)\n",
    "        \n",
    "        # attention2参数\n",
    "        self.w_omega2 = nn.Parameter(torch.Tensor(embed_dim, embed_dim))\n",
    "        self.u_omega2 = nn.Parameter(torch.Tensor(embed_dim, 1))\n",
    "        nn.init.uniform_(self.w_omega2, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.u_omega2, -0.1, 0.1)\n",
    "        \n",
    "    def attention_net(self, x):   #x:[batch_size, seq_len, hidden_dim]\n",
    "        u = torch.tanh(torch.matmul(x, self.w_omega))          #[batch, seq_len, hidden_dim]\n",
    "        att = torch.matmul(u, self.u_omega)                           #[batch, seq_len, 1]\n",
    "        att_score = F.softmax(att, dim=1)\n",
    "        scored_x = x * att_score                           #[batch, seq_len, hidden_dim]\n",
    "        feat = torch.sum(scored_x, dim=1)          #[batch, hidden_dim]\n",
    "        return feat\n",
    "        \n",
    "    def attention_net2(self, x):   \n",
    "        u = torch.tanh(torch.matmul(x, self.w_omega2))\n",
    "        att = torch.matmul(u, self.u_omega2)\n",
    "        att_score = F.softmax(att, dim=1)\n",
    "        scored_x = x * att_score\n",
    "        feat = torch.sum(scored_x, dim=1) \n",
    "        return feat\n",
    "\n",
    "    def forward(self, shop_idxs, edge_index, userId, poiId, userHistory, userHistoryLength): \n",
    "        x = self.shop_embedding(shop_idxs)\n",
    "        x = self.bn(x)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.active(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.active(x)\n",
    "        region_embedding = nn.Embedding(self.shop_size, self.shop_embed_size).cuda()\n",
    "        region_embedding.weight.data.copy_(x)\n",
    "        \n",
    "        currentPOI = self.shop_embedding(poiId)\n",
    "        # 不等长lstm的batch输入处理，batch按照长度由大到小排序\n",
    "        userHistory_lengths, idx = userHistoryLength.sort(0, descending=True)\n",
    "        _, un_idx = torch.sort(idx, dim=0)\n",
    "        userHistory = userHistory[idx]\n",
    "        userHistory_POI = self.shop_embedding(userHistory)\n",
    "        print(\"userHistory_POI\")\n",
    "        print(userHistory_POI)\n",
    "        userHistory_POI_packed_input = pack_padded_sequence(input=userHistory_POI, lengths=userHistory_lengths, batch_first=True)\n",
    "        # hidden_state集合；userHistory_POI_packed_out是[batch_size*seq_len,embed_dim]的tensor;_[0]是代表每个seq最后一个hiddenState构成的tensor,[1, batch_size, embedding]\n",
    "        userHistory_POI_packed_out, (h_n, c_n) = self.lstm1(userHistory_POI_packed_input)\n",
    "        # userHistory_POI_packed_out解压，仍按照长度由大到小排序, [batch_size,seq_len, embed_dim];_是一个tensor，每个元素代表每个seq的真实长度\n",
    "        userHistory_POI_out, _ = pad_packed_sequence(userHistory_POI_packed_out, batch_first=True)\n",
    "        # userHistory_POI_out恢复正常顺序，[batch_size,seq_len, embed_dim]，其后有补0\n",
    "        userHistory_POI_out = torch.index_select(userHistory_POI_out, 0, un_idx)\n",
    "        print(\"userHistory_POI_out\")\n",
    "        print(userHistory_POI_out)\n",
    "        # 使用attention机制对seq的所有hidden_state加权, long_userPref_POI[batch_size, embed_dim]\n",
    "        userAvePOIPref_attn = self.attention_net(userHistory_POI_out)\n",
    "        userAvePOIPref = torch.cat([userAvePOIPref_attn, currentPOI], -1)\n",
    "        # 每个seq的最后一个hidden_state构成的集合,short_userPref_POI[batch_size, embed_dim]\n",
    "        userShortPOIPref = torch.index_select(h_n, 1, un_idx)\n",
    "        userShortPOIPref = torch.squeeze(userShortPOIPref, 0)\n",
    "        # userPref1 = torch.cat([userShortPOIPref, currentPOI], -1)\n",
    "        \n",
    "        currentPOIRegion = region_embedding(poiId)\n",
    "        userHistory_Region = region_embedding(userHistory)\n",
    "        userHistory_Region_packed_input = pack_padded_sequence(input=userHistory_Region, lengths=userHistory_lengths, batch_first=True)\n",
    "        userHistory_Region_packed_out, (h_nr, c_nr) = self.lstm2(userHistory_Region_packed_input)\n",
    "        userHistory_Region_out, _ = pad_packed_sequence(userHistory_Region_packed_out, batch_first=True)\n",
    "        userHistory_Region_out = torch.index_select(userHistory_Region_out, 0, un_idx)\n",
    "        userAveRegionPref_attn = self.attention_net2(userHistory_Region_out)\n",
    "        # userLongRegionPref = torch.cat([userLongRegionPref_attn, currentPOI], -1)\n",
    "        userShortRegionPref = torch.index_select(h_nr, 1, un_idx)\n",
    "        userShortRegionPref = torch.squeeze(userShortRegionPref, 0)\n",
    "\n",
    "        output = self.fc2(userAvePOIPref)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=opt.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
